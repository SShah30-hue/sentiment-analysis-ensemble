{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907d9a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import AUROC\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, RobertaModel, RobertaTokenizer, get_linear_schedule_with_warmup\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional import accuracy, f1, auroc\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da900a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac15c46",
   "metadata": {},
   "source": [
    "### Audio embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a926649",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wav_PATH = \"C:/Data/Sentiment Analysis/MELD/Processed/wav2vec/embeddings_v2/train_wav.pt\"\n",
    "dev_wav_PATH = \"C:/Data/Sentiment Analysis/MELD/Processed/wav2vec/embeddings_v2/dev_wav.pt\"\n",
    "test_wav_PATH = \"C:/Data/Sentiment Analysis/MELD/Processed/wav2vec/embeddings_v2/test_wav.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4efa094",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wav_embeddings = torch.load(train_wav_PATH, map_location=device)\n",
    "dev_wav_embeddings = torch.load(dev_wav_PATH, map_location=device)\n",
    "test_wav_embeddings = torch.load(test_wav_PATH, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad0060",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wav_embeddings[\"embeddings\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a10715",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fe4635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of data\n",
    "train_dataset = []\n",
    "\n",
    "# i is the index of arrays\n",
    "for i in range(len(train_wav_embeddings[\"fileID\"])):\n",
    "    train_dataset.append({\n",
    "        \"fileID\": train_wav_embeddings[\"fileID\"][i], \n",
    "        \"label\": train_wav_embeddings[\"labels\"][i], \n",
    "        \"wav_embeddings\": train_wav_embeddings[\"embeddings\"][i][0],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a839b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d45909",
   "metadata": {},
   "source": [
    "## Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9968d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of data\n",
    "dev_dataset = []\n",
    "\n",
    "# i is the index of arrays\n",
    "for i in range(len(dev_wav_embeddings[\"fileID\"])):\n",
    "    dev_dataset.append({\n",
    "        \"fileID\": dev_wav_embeddings[\"fileID\"][i], \n",
    "        \"label\": dev_wav_embeddings[\"labels\"][i], \n",
    "        \"wav_embeddings\": dev_wav_embeddings[\"embeddings\"][i][0],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b398793",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dev_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29db330f",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e375a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of data\n",
    "test_dataset = []\n",
    "\n",
    "# i is the index of arrays\n",
    "for i in range(len(test_wav_embeddings[\"fileID\"])):\n",
    "    test_dataset.append({\n",
    "        \"fileID\": test_wav_embeddings[\"fileID\"][i], \n",
    "        \"label\": test_wav_embeddings[\"labels\"][i], \n",
    "        \"wav_embeddings\": test_wav_embeddings[\"embeddings\"][i][0],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d957da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d2778c",
   "metadata": {},
   "source": [
    "## Encapsulating all data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc177838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, train_dataset, test_dataset, dev_dataset, batch_size=16):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train = train_dataset\n",
    "        self.dev = dev_dataset\n",
    "        self.test = test_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "          self.train,\n",
    "          batch_size=self.batch_size,\n",
    "          shuffle=False,\n",
    "          num_workers=0\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "          self.dev,\n",
    "          batch_size=self.batch_size,\n",
    "          num_workers=0\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "          self.test,\n",
    "          batch_size=self.batch_size,\n",
    "          num_workers=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6c0056",
   "metadata": {},
   "source": [
    "#### Instance for class DataModule "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fdae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "data_module = DataModule(train_dataset, test_dataset, dev_dataset,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3618a6",
   "metadata": {},
   "source": [
    "### Modelling Prep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a9393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset[0][\"wav_embeddings\"].size()\n",
    "test_dataset[0][\"wav_embeddings\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ed578",
   "metadata": {},
   "source": [
    "## Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906cfceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = nn.Linear(768, 668)\n",
    "        self.linear_2 = nn.Linear(668, 568)\n",
    "        self.linear_3 = nn.Linear(568, 468)\n",
    "        self.linear_4 = nn.Linear(468, 368)\n",
    "        self.linear_5 = nn.Linear(368, 256)\n",
    "        self.classifier = nn.Linear(256, n_classes)\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "\n",
    "    def forward(self, wav_embeddings, labels=None):\n",
    "\n",
    "        output = wav_embeddings\n",
    "        #print(output)\n",
    "        #print(output.shape)\n",
    "        output = self.linear_1(output)\n",
    "        output = self.linear_2(output)\n",
    "        output = self.linear_3(output)\n",
    "        output = self.linear_4(output)\n",
    "        output = self.linear_5(output)\n",
    "        output = self.classifier(output)\n",
    "        output = torch.softmax(output, dim=1)\n",
    "        loss = 0\n",
    "        \n",
    "        #Print to debug\n",
    "        #print(output.shape)\n",
    "        #print(labels)\n",
    "        \n",
    "        if labels is not None:\n",
    "            labels = labels.flatten() ##\n",
    "            loss = self.criterion(output, labels)\n",
    "        \n",
    "        return loss, output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        audio_embeddings = batch[\"wav_embeddings\"]\n",
    "        labels = batch[\"label\"]\n",
    "        loss, outputs = self(audio_embeddings, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        audio_embeddings = batch[\"wav_embeddings\"]\n",
    "        labels = batch[\"label\"]\n",
    "        loss, outputs = self(audio_embeddings, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        audio_embeddings = batch[\"wav_embeddings\"]\n",
    "        labels = batch[\"label\"]\n",
    "        loss, outputs = self(audio_embeddings, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "\n",
    "        labels = []\n",
    "        predictions = []\n",
    "\n",
    "        for output in outputs:\n",
    "            for out_labels in output[\"labels\"].detach().cpu():\n",
    "                labels.append(out_labels)\n",
    "            for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "                predictions.append(out_predictions)\n",
    "        \n",
    "        labels = torch.stack(labels).int()\n",
    "        predictions = torch.stack(predictions)\n",
    "        pred = torch.argmax(predictions, dim=1)\n",
    "        \n",
    "        train_acc = accuracy(pred, labels, num_classes=3)\n",
    "        #print(\"Label:\", labels)\n",
    "        #print(\"Prediction:\", pred)\n",
    "        print(\"Training Accuracy:\", train_acc)\n",
    "        \n",
    "        label = labels.flatten()\n",
    "        auroc = AUROC(num_classes=3)\n",
    "        auroc = auroc(predictions, label)\n",
    "        print(\"AUROC:\", auroc)\n",
    "\n",
    "    def configure_optimizers(self): #configuring the optimizers\n",
    "\n",
    "        optimizer = AdamW(self.parameters(), lr=2e-5)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "          optimizer,\n",
    "          num_warmup_steps=self.n_warmup_steps,\n",
    "          num_training_steps=self.n_training_steps\n",
    "\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=dict(\n",
    "                scheduler=scheduler,\n",
    "                interval='step'\n",
    "            )\n",
    "\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d817be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch=len(train_dataset) // BATCH_SIZE\n",
    "total_training_steps = steps_per_epoch * N_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b9f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1/5 of the training steps as warm-up\n",
    "warmup_steps = total_training_steps // 5\n",
    "warmup_steps, total_training_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187bc6ee",
   "metadata": {},
   "source": [
    "#### Instance for modelling class Tagger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tagger(\n",
    "  n_classes=3,\n",
    "  n_warmup_steps=warmup_steps,\n",
    "  n_training_steps=total_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f4dfb",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4539c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checkpointing that saves the best model (based on validation loss)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"C:/Users/id301281/NLP/NLU/MELD/Fusion\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06f04ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#And early stopping triggers when the loss hasn’t improved for the last 30 epochs\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f2db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting training\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    max_epochs=N_EPOCHS,\n",
    "    gpus=1,\n",
    "    progress_bar_refresh_rate=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59afb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68dc7a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf425e7",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef85a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7322f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading best trained model\n",
    "trained_model = Tagger.load_from_checkpoint(\n",
    "  trainer.checkpoint_callback.best_model_path,\n",
    "  n_classes=3\n",
    ")\n",
    "\n",
    "trained_model.eval()\n",
    "trained_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [] \n",
    "labels = []\n",
    "\n",
    "for i in test_dataset:\n",
    "    \n",
    "    wav_embeddings = i[\"wav_embeddings\"].unsqueeze(0)\n",
    "    actual = i['label']    \n",
    "    pred1 = trained_model(wav_embeddings)\n",
    "    pred2 = torch.argmax(pred1[1], dim=1).squeeze().tolist()\n",
    "    \n",
    "    preds.append(pred2)\n",
    "    labels.append(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d845cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_map = {\n",
    "    \n",
    "    \"negative\": \"0\",\n",
    "    \"neutral\": \"1\",\n",
    "    \"positive\": \"2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ea686",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels, preds, target_names=encode_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0363b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\")\n",
    "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "  plt.ylabel('True sentiment')\n",
    "  plt.xlabel('Predicted sentiment');\n",
    "\n",
    "cm = confusion_matrix(labels, preds)\n",
    "df_cm = pd.DataFrame(cm, index=encode_map, columns=encode_map)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6599a7f4",
   "metadata": {},
   "source": [
    "### Storing test predictions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b952d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading best trained model\n",
    "trained_model = Tagger.load_from_checkpoint(\n",
    "  trainer.checkpoint_callback.best_model_path,\n",
    "  n_classes=3\n",
    ")\n",
    "\n",
    "trained_model.eval()\n",
    "trained_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dae5c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = trained_model(test_dataset[0][\"wav_embeddings\"].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preed = test_prediction[1].flatten().numpy()\n",
    "for i, predd in zip(encode_map, preed):\n",
    "    print(f\"{i}: {predd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e2577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]['wav_embeddings'].unsqueeze(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390c9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = { \"fileID_w2vb\": [], \"Negative_w2vb\" : [], \"Neutral_w2vb\" : [], \"Positive_w2vb\" : [], \"predicted_w2vb\": [], \"actual_w2vb\": []}\n",
    "\n",
    "for i in test_dataset:\n",
    "    \n",
    "    wav_embeddings = i[\"wav_embeddings\"].unsqueeze(0)\n",
    "    actual = i['label']\n",
    "    fileID = i['fileID']\n",
    "    \n",
    "    pred1 = trained_model(wav_embeddings)\n",
    "    pred2 = torch.argmax(pred1[1], dim=1).squeeze().tolist()\n",
    "    pred3 = pred1[1].flatten().numpy()\n",
    "    \n",
    "    test_predictions[\"predicted_w2vb\"].append(pred2)\n",
    "    test_predictions[\"Negative_w2vb\"].append(pred3[0])\n",
    "    test_predictions[\"Neutral_w2vb\"].append(pred3[1])\n",
    "    test_predictions[\"Positive_w2vb\"].append(pred3[2])\n",
    "    test_predictions[\"actual_w2vb\"].append(actual)\n",
    "    test_predictions[\"fileID_w2vb\"].append(fileID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404126ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=test_predictions)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c209604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving to csv\n",
    "df.to_csv(r\"C:\\Data\\Sentiment Analysis\\MELD\\ensemble preds\\wav2vec\\v2\\test_preds_w2vb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677eb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f94e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-GoogleSTT]",
   "language": "python",
   "name": "conda-env-.conda-GoogleSTT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
