{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c3e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from typing import Dict, List, Optional, Union\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (Wav2Vec2PreTrainedModel,Wav2Vec2Model)\n",
    "from typing import Any, Dict, Union\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoConfig, Wav2Vec2Processor, Wav2Vec2Model\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68689c1d",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add73f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"train\": \"C:/Data/Sentiment Analysis/MELD/Processed/wav2vec/wav_train.csv\", \n",
    "    \"dev\": \"C:/Data/Sentiment Analysis/MELD/Processed/wav2vec/wav_dev.csv\",\n",
    "    \"test\": \"C:/Data/Sentiment Analysis/MELD/Processed/wav2vec/wav_test.csv\",\n",
    "    \n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"dev\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "print(train_dataset)\n",
    "print(eval_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c4591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to specify the input and output column\n",
    "input_column = \"path\"\n",
    "output_column = \"sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be0961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to distinguish the unique labels in our SER dataset\n",
    "label_list = train_dataset.unique(output_column)\n",
    "label_list.sort()  # Let's sort it for determinism\n",
    "num_labels = len(label_list)\n",
    "print(f\"A classification problem with {num_labels} classes: {label_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60e4124",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"facebook/wav2vec2-large-960h\"\n",
    "pooling_mode = \"mean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    label2id={label: i for i, label in enumerate(label_list)},\n",
    "    id2label={i: label for i, label in enumerate(label_list)},\n",
    "    finetuning_task=\"wav2vec2_clf\",\n",
    ")\n",
    "setattr(config, 'pooling_mode', pooling_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10130356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b6b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(model_name_or_path)\n",
    "target_sampling_rate = processor.feature_extractor.sampling_rate\n",
    "print(f\"The target sampling rate: {target_sampling_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddcd597",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c981305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(path):\n",
    "    speech_array, sampling_rate = torchaudio.load(path)\n",
    "    resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n",
    "    speech = resampler(speech_array).squeeze().numpy()\n",
    "    return speech\n",
    "\n",
    "def label_to_id(label, label_list):\n",
    "\n",
    "    if len(label_list) > 0:\n",
    "        return label_list.index(label) if label in label_list else -1\n",
    "\n",
    "    return label\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]\n",
    "    target_list = [label_to_id(label, label_list) for label in examples[output_column]]\n",
    "\n",
    "    result = processor(speech_list, sampling_rate=target_sampling_rate)\n",
    "    result[\"labels\"] = list(target_list)\n",
    "    \n",
    "    #print(len(speech_list))\n",
    "    #print(speech_list)\n",
    "    #print(len(result))\n",
    "    #print(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e085435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess_function(train_dataset[:2]) #debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a493db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batch_size=100,\n",
    "    batched=True,\n",
    "    num_proc=1\n",
    ")\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batch_size=100,\n",
    "    batched=True,\n",
    "    num_proc=1\n",
    ")\n",
    "test_dataset = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batch_size=100,\n",
    "    batched=True,\n",
    "    num_proc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782fe632",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "print(f\"Training input_values: {train_dataset[idx]['input_values']}\")\n",
    "print(f\"Training labels: {train_dataset[idx]['labels']} - {train_dataset[idx]['sentiment']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be485895",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset[idx]['input_values'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fa8a3b",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8de5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        #self.num_labels = config.num_labels\n",
    "        self.pooling_mode = config.pooling_mode\n",
    "        self.config = config\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        #self.classifier = Wav2Vec2ClassificationHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def merged_strategy(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            mode=\"max\"\n",
    "    ):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "            labels=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs[0]\n",
    "        #print(len(hidden_states))\n",
    "        #print(hidden_states.shape)\n",
    "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "        #print(hidden_states.shape)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a8586",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed584aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path,config=config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb2bd8f",
   "metadata": {},
   "source": [
    "### Extracting train embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c9a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {'embeddings' : [], 'labels' : [], 'fileID' : []}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_values in train_dataset['input_values']:\n",
    "        tensor = torch.FloatTensor([input_values]).to(device)\n",
    "        results = model(tensor)\n",
    "        embeddings[\"embeddings\"].append(results)\n",
    "\n",
    "for labels in train_dataset['labels']:\n",
    "    label = labels\n",
    "    embeddings[\"labels\"].append(label)\n",
    "\n",
    "for file in train_dataset['name']:\n",
    "    fileID = file\n",
    "    embeddings[\"fileID\"].append(fileID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings['embeddings'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e754785",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings['embeddings'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d210d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings['embeddings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c58c972",
   "metadata": {},
   "source": [
    "#### Saving and loading tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e38d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Data/Sentiment Analysis/MELD/Processed/wav2vec/embeddings_v2/train_wav_large.pt\"\n",
    "torch.save(embeddings, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f059de",
   "metadata": {},
   "outputs": [],
   "source": [
    "embgs = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6467513",
   "metadata": {},
   "outputs": [],
   "source": [
    "embgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f308e8af",
   "metadata": {},
   "source": [
    "### Extracting dev embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b8062",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {'embeddings' : [], 'labels' : [], 'fileID' : []}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_values in eval_dataset['input_values']:\n",
    "        tensor = torch.FloatTensor([input_values]).to(device)\n",
    "        results = model(tensor)\n",
    "        embeddings[\"embeddings\"].append(results)\n",
    "\n",
    "for labels in eval_dataset['labels']:\n",
    "    label = labels\n",
    "    embeddings[\"labels\"].append(label)\n",
    "\n",
    "for file in eval_dataset['name']:\n",
    "    fileID = file\n",
    "    embeddings[\"fileID\"].append(fileID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2e20c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings['embeddings'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139ad8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings['embeddings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47b6535",
   "metadata": {},
   "source": [
    "#### Saving and loading tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28115c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Data/Sentiment Analysis/MELD/Processed/wav2vec/embeddings_v2/dev_wav_large.pt\"\n",
    "torch.save(embeddings, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba0a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embgs = torch.load(PATH)\n",
    "embgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad11014",
   "metadata": {},
   "source": [
    "### Extracting test embeddings  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7032d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {'embeddings' : [], 'labels' : [], 'fileID' : []}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_values in test_dataset['input_values']:\n",
    "        tensor = torch.FloatTensor([input_values]).to(device)\n",
    "        results = model(tensor)\n",
    "        embeddings[\"embeddings\"].append(results)\n",
    "\n",
    "for labels in test_dataset['labels']:\n",
    "    label = labels\n",
    "    embeddings[\"labels\"].append(label)\n",
    "\n",
    "for file in test_dataset['name']:\n",
    "    fileID = file\n",
    "    embeddings[\"fileID\"].append(fileID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d397c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings['embeddings'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings['embeddings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f586991",
   "metadata": {},
   "source": [
    "#### Saving and loading tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e77c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Data/Sentiment Analysis/MELD/Processed/wav2vec/embeddings_v2/test_wav_large.pt\"\n",
    "torch.save(embeddings, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e31e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embgs = torch.load(PATH)\n",
    "embgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c978f36",
   "metadata": {},
   "source": [
    "##  Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83019612",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][\"input_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461895a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.FloatTensor([train_dataset[0]['input_values']]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640f2eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aea055",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731ba4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-GoogleSTT]",
   "language": "python",
   "name": "conda-env-.conda-GoogleSTT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
